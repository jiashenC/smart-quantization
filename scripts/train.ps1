# forward + backward
python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 4 --max_epochs 100 --compress smart --gradient_clip_val 1.0 --num_bits_main 6 --num_bits_outlier 8 --name "smart-stochastic-[forward,backward,loss,weight,grad,momentum]-[6-8]" --resume_from_checkpoint "./lightning_logs_bert/smart-stochastic-[forward,backward,loss,weight,grad,momentum]-[6-8]/version_0/checkpoints/epoch=1-step=367.ckpt"
python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress fp8 --name "fp8-[forward,backward,loss,weight,grad,momentum]" --resume_from_checkpoint "./lightning_logs_bert/fp8-[forward,backward,loss,weight,grad,momentum]/version_0/checkpoints/epoch=11-step=2207.ckpt"
python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress s2fp8 --name "s2fp8-[forward,backward,loss,weight,grad,momentum]" --resume_from_checkpoint "./lightning_logs_bert/s2fp8-[forward,backward,loss,weight,grad,momentum]/version_0/checkpoints/epoch=2-step=551.ckpt"
python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress fp16 --name "fp16-[forward,backward,loss,weight,grad,momentum]" --resume_from_checkpoint "./lightning_logs_bert/fp16-[forward,backward,loss,weight,grad,momentum]/version_0/checkpoints/epoch=2-step=551.ckpt"
python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress bf16 --name "bf16-[forward,backward,loss,weight,grad,momentum]" --resume_from_checkpoint "./lightning_logs_bert/bf16-[forward,backward,loss,weight,grad,momentum]/version_0/checkpoints/epoch=15-step=2943.ckpt"
python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress fp32 --name "fp32-[forward,backward,loss,weight,grad,momentum]" --resume_from_checkpoint "./lightning_logs_bert/fp32-[forward,backward,loss,weight,grad,momentum]/version_0/checkpoints/epoch=1-step=367.ckpt"
python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress smart --gradient_clip_val 1.0 --num_bits_main 5 --num_bits_outlier 7 --name "smart-stochastic-[forward,backward,loss,weight,grad,momentum]-[5-7]" --resume_from_checkpoint "./lightning_logs_bert/smart-stochastic-[forward,backward,loss,weight,grad,momentum]-[5-7]/version_0/checkpoints/epoch=8-step=1655.ckpt"
python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress smart --gradient_clip_val 1.0 --num_bits_main 4 --num_bits_outlier 6 --name "smart-stochastic-[forward,backward,loss,weight,grad,momentum]-[4-6]" --resume_from_checkpoint "./lightning_logs_bert/smart-stochastic-[forward,backward,loss,weight,grad,momentum]-[4-6]/version_0/checkpoints/epoch=1-step=367.ckpt"
python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress smart --gradient_clip_val 1.0 --num_bits_main 3 --num_bits_outlier 5 --name "smart-stochastic-[forward,backward,loss,weight,grad,momentum]-[3-5]" --resume_from_checkpoint "./lightning_logs_bert/smart-stochastic-[forward,backward,loss,weight,grad,momentum]-[3-5]/version_0/checkpoints/epoch=2-step=551.ckpt"

# # forward only
# python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress smart --gradient_clip_val 1.0 --no_compress_backward --num_bits_main 6 --num_bits_outlier 8 --name "smart-stochastic-[backward,loss,weight,grad,momentum]-[6-8]"
# python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress fp8 --no_compress_backward --name "fp8-[forward,loss,weight,grad,momentum]"
# python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress s2fp8 --no_compress_backward --name "s2fp8-[forward,loss,weight,grad,momentum]"
# python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress fp16 --no_compress_backward --name "fp16-[forward,loss,weight,grad,momentum]"
# python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress bf16 --no_compress_backward --name "bf16-[forward,loss,weight,grad,momentum]"
# python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress smart --gradient_clip_val 1.0 --no_compress_backward --num_bits_main 5 --num_bits_outlier 7 --name "smart-stochastic-[backward,loss,weight,grad,momentum]-[5-7]"
# python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress smart --gradient_clip_val 1.0 --no_compress_backward --num_bits_main 4 --num_bits_outlier 6 --name "smart-stochastic-[backward,loss,weight,grad,momentum]-[4-6]"
# python ./train.py --accelerator ddp --gpus -1 --model bert --dataset glue --batch_size 10 --val_batch_size 2 --max_epochs 100 --compress smart --gradient_clip_val 1.0 --no_compress_backward --num_bits_main 3 --num_bits_outlier 5 --name "smart-stochastic-[backward,loss,weight,grad,momentum]-[3-5]"
